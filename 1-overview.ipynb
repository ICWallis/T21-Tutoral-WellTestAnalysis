{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "# Tutoral outline\n",
    "\n",
    "\n",
    "_1-overview.ipynb_\n",
    "\n",
    "**1. Introductory concepts**\n",
    "\n",
    "    1.1 When we say 'geothermal' we mean...\n",
    "\n",
    "    1.2 Introduction to geothermal wells\n",
    "\n",
    "    1.3 Introduction to completion tests\n",
    "\n",
    "\n",
    "**2. Import and munge test data**\n",
    "    \n",
    "    2.1 Pump (flowrate) data\n",
    "        \n",
    "    2.1.1 Import flowrate data using Pandas\n",
    "\n",
    "    2.1.2 Rename columns\n",
    "\n",
    "    2.1.3 Convert datetime string into Python datetime \n",
    "\n",
    "    2.1.4 Add a flowrate field in our desired units\n",
    "\n",
    "    2.2 Pressure, temperature, spinner tool data\n",
    "\n",
    "    2.1.1 Import pts data using Pandas\n",
    "\n",
    "    2.2.2 Rename selected column headers\n",
    "\n",
    "    2.2.3 Drop units row\n",
    "\n",
    "    2.2.4 Recognise bad datetime data & how to handle excel datetime floats\n",
    "\n",
    "    2.2.5 Drop unwanted columns from dataframe\n",
    "\n",
    "    2.2.6 Correct the data types\n",
    "\n",
    "**3. Overview the completion test data**\n",
    "\n",
    "\n",
    "_2-temprature.ipynb_\n",
    "\n",
    "**4. Introductory concepts**\n",
    "\n",
    "    4.1 The temperature when?\n",
    "\n",
    "    4.2 Uses of temperature logs in geothermal\n",
    "\n",
    "**5. Extract and evaluate temperature from completion test data**\n",
    "\n",
    "    5.1 Use functions to import and mudge data\n",
    "\n",
    "    5.2 Extract the desired temperature log\n",
    "\n",
    "    5.2.1 Use ipywidget sliders to interactively select desired data\n",
    "\n",
    "    5.3 Use XX to generate a new dataframe of the desired data\n",
    "\n",
    "    5.4 Evaulate the temprature log\n",
    "\n",
    "_3-injectivity.ipynb_\n",
    "\n",
    "_2-feedzones.ipynb_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "XXX\n",
    "\n",
    "## 1.1 When we say 'geothermal' we mean...\n",
    "\n",
    "conventional geothermal resources where fluid is rising buoyantly through the crust to form a reservoir that can be intersected by drilling. Where there is sufficient temperature to discharge with the energy of boil or hydrostatic head, wells will self-discharge. Otherwise, wells are pumped. The liquid reservoir is typically hot hydrostatic (i.e., at a lower pressure than the regional cold hydrostatic gradient). We do not discuss engineered geothermal systems, which are also referred to as 'hot dry rock', or the low-temperature sedimentary aquifer systems. However, the testing methods used in those settings are similar to what is described in this tutorial. \n",
    "\n",
    "### Include the temperature range table with a note for self-discharging vs pumped\n",
    "\n",
    "## 1.2 Introduction to geothermal wells\n",
    "\n",
    "Long open-hole lengths with perforated liners (with a photo of the liner)\n",
    "\n",
    "Flow because of boiling (rather than from pressure)\n",
    "\n",
    "Drilled, completion test, heated, production test \n",
    "\n",
    "Photo of the tool\n",
    "\n",
    "## 1.3 Introduction to completion tests\n",
    "\n",
    "A completion test is ... \n",
    "\n",
    "\n",
    "\n",
    "**Katie to finish text?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# 2. Import and munge test data\n",
    "\n",
    "We are working with two sources of data:\n",
    "\n",
    "1. Pumps operating at the surface (datetime and flow rate)\n",
    "2. PTS tool in the well (datetime, pressure, temperature, spinner, other tool information)\n",
    "\n",
    "The following steps take the raw data that has been provided as excel files, imports them as a Pandas dataframe (one called flowrate and the other called pts), and then do some manipulation to generate a nice clean dataset that we can work with.\n",
    "\n",
    "We will see that data format and data quality are common challenges, especially when datetime is involved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 2.1. Pump (flowrate) data\n",
    "\n",
    "We will turn the excel data into a Pandas dataframe that includes:\n",
    "- A datetime field we can use with the tool data\n",
    "- Flowrate in the units we need\n",
    "\n",
    "### 2.1.1 Import flowrate data using Pandas\n",
    "\n",
    "We read the data in with the header location defined as row 1. This automatically skips row 0 which just contains some text that we donâ€™t want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowrate = pd.read_excel(r'Data-FlowRate.xlsx', header=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful Pandas methods\n",
    "\n",
    "We will regularly use a set of Pandas methods to check our data as we munge it\n",
    "\n",
    "\n",
    "Display the first few rows of the dataframe\n",
    "> df.head() _or_ df.head(n) _to show n rows_\n",
    "\n",
    "Display the last few rows of the dataframe\n",
    "> df.tail() _or_ df.tail(n) _to show n rows_\n",
    "\n",
    "Useful information like the dataframe shape and the data types\n",
    "> df.info() \n",
    "\n",
    "A list of the column headers\n",
    "> df.columns \n",
    "\n",
    "Data types in each coloumn\n",
    "> df.dtypes\n",
    "\n",
    "Shape of the dataframe\n",
    "> df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Rename coloums\n",
    "\n",
    "We do this to remove spaces from the column headers. \n",
    "\n",
    "This step is not compulsory, but the presence of spaces in the column names dictates the code format. Once a column has been created, we can interchangeably use the flowrate.raw_datetime or flowrate\\['raw_datetime'\\] notation. If there are spaces in the column names, the df.column_name notation will not work and we must use the df\\['column_name'\\] format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowrate.columns = ['raw_datetime','flow_Lpm'] \n",
    "\n",
    "flowrate.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Convert datetime string into Python datetime \n",
    "\n",
    "The datetime is in [ISO format](https://en.wikipedia.org/wiki/ISO_8601) and, because it includes the timezone, this is the most globally useful datetime format. \n",
    "\n",
    "> 2020-12-11T06:00:00+13:00 \n",
    "\n",
    "where T indicates the time data and +13:00 is the time shift between UTC and New Zealand where the pumps were recording data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowrate.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the output above and note that the _raw_datetime_ data type is _object_ and not _datetime_\n",
    "\n",
    "We will use a two-step process to parse this date:\n",
    "\n",
    "1. Convert it to a datetime object with the default UTC timezone\n",
    "2. Convert the datetime object to local time in New Zealand so it can be combined with the pts tool data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a method that recognises the ISO format to parse the datetime\n",
    "from datetime import datetime\n",
    "\n",
    "list = []\n",
    "for date in flowrate['raw_datetime']:\n",
    "    newdate = datetime.fromisoformat(date)\n",
    "    list.append(newdate)\n",
    "flowrate['ISO_datetime'] = list\n",
    "\n",
    "flowrate.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method fromisoformate() is new in version Python 3.7\n",
    "\n",
    "More information can be found [here](https://docs.python.org/3/library/datetime.html) and [here](https://pythontic.com/datetime/date/fromisoformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowrate.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the data type for the ISO_datetime column is a UTC datetime: datetime64\\[ns, UTC+13:00\\]\n",
    "\n",
    "But we need it in local NZ time to combine with the PTS tool data. Luckily, the datetime.strftime method recognises the timezone in the datetime object and does the conversion for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "for date in flowrate.ISO_datetime:\n",
    "    newdate = pd.to_datetime(datetime.strftime(date,'%Y-%m-%d %H:%M:%S'))\n",
    "    list.append(newdate)\n",
    "flowrate['datetime'] = list\n",
    "\n",
    "flowrate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowrate.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a Python datetime object that we can combine with the PTS data which does not have timezone information.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Add a flowrate field in our desired units\n",
    "\n",
    "Conversions for typical geothermal flowrate units:\n",
    "\n",
    "- L/sec to t/hr 3.6\n",
    "- L/min to t/hr 0.060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowrate['flow_tph'] = flowrate.flow_Lpm * 0.060\n",
    "\n",
    "flowrate.head(2) # 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Trim excess coloumns from the dataframe\n",
    "\n",
    "Now we just drop the columns that we don't want from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowrate.drop(columns = ['raw_datetime', 'flow_Lpm', 'ISO_datetime'], inplace = True)\n",
    "\n",
    "flowrate.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inplace = True\n",
    "\n",
    "Each time we do something something like drop columns, we generate a new Pandas dataframe. The argument inplace=True overwrites the old dataframe with the new one. We could have achieved the same thing with assignment:\n",
    "\n",
    "> flowrate = flowrate.drop(columns = \\['raw_datetime', 'flow_Lpm', 'ISO_datetime'\\])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2.2 Pressure, temperature, spinner tool data\n",
    "\n",
    "We will use a similar process as above to import and munge the pressure, temperature, and spinner (PTS) tool data. However, the PTS dataset includes an addional challenge generated by bad data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Import pts data using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = pd.read_excel(r'Data-PTS.xlsx') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the cell below, you will see that we have a lot of data in the pts dataframe (101294 rows and 26 columns). \n",
    "\n",
    "Efficient handling of large datasets is one of the reasons why Python is better for these kinds of analysis than excel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Rename selected coloumn headders\n",
    "\n",
    "The dataframe has a row of units below the header row that we need to remove. But because we want to know the units of our data, we will rename key columns to remove spaces and include units before dropping the row containing the units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary method for renaming some coloums\n",
    "dict = {\n",
    "    'DEPTH':'depth_m',\n",
    "    'SPEED': 'speed_mps',\n",
    "    'Cable Weight': 'cweight_kg',\n",
    "    'WHP': 'whp_barg',\n",
    "    'Temperature': 'temp_degC',\n",
    "    'Pressure': 'pressure_bara',\n",
    "    'Frequency': 'frequency_hz'\n",
    "}\n",
    "\n",
    "pts.rename(columns=dict, inplace=True)\n",
    "\n",
    "pts.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Drop units row\n",
    "\n",
    "Note that we are using the index value to drop the row of units and then we remake the index. Therefore, if we run this cell over and over, it will keep dropping the first row of data. \n",
    "\n",
    "If you are unsure what state your dataframe is in or how many times you have run the cell below, it is useful to run the process from the start again (i.e., go back to step _2.1.1 Import pts data using Pandas_ and work down from there). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop units row using the index value of that row\n",
    "pts.drop(0, inplace=True)\n",
    "\n",
    "# re-index the dataframe\n",
    "pts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# WARNING if you run this cell again, it will drop the first line of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Recognise bad datetime data & how to handle excel datetime floats\n",
    "\n",
    "If you investigate the _Time_ column in our pts dataframe, you'll find that the hours are all set to 00 despite the test running for nearly 12 hours and starting at around 9 in the morning. Unfortunately, these kinds of issues with bad data are common and they are why it's important to QA/QC of data when it is acquired. \n",
    "\n",
    "We will (reluctantly) use the excel timestamp to generate a new datetime.\n",
    "\n",
    "An excel timestamp is a floating-point number that reflects the number of days (or part thereof) since a specified start day. There are two systems that start counting from different dates (1900 and 1904) and some fundamental bugs in the way they are implemented. The start date is stored in the excel workbook and we need to know which system is used to correctly generate a datetime. Check out [this documentation](https://openpyxl.readthedocs.io/en/stable/datetime.html) for more information on the challenges of excel timestamps. **The short version is that we should avoid using excel float time whenever possible.** Unfortunately, it is not possible here because of the bad data.\n",
    "\n",
    "Because we have an xlsx file, will use the openpyxl package to determine the start date in the workbook and parse the excel float to datetime. If you have an xls file, then the xlrd module may be used to achieve the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first check the start day system in the workbook\n",
    "import openpyxl\n",
    "\n",
    "wb = openpyxl.load_workbook(filename=\"Data-PTS.xlsx\")\n",
    "if wb.epoch == openpyxl.utils.datetime.CALENDAR_WINDOWS_1900: \n",
    "    print(\"This workbook is using the 1900 date system.\")\n",
    "if wb.epoch == openpyxl.utils.datetime.CALENDAR_MAC_1904: \n",
    "    print(\"This workbook is using the 1904 date system.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have the 1900 date system, we can use the default arguments in the from_excel method in the openpyxl utilities. Refer to [the docs](https://openpyxl.readthedocs.io/en/stable/api/openpyxl.utils.datetime.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "for date in pts.Timestamp:\n",
    "    newdate = openpyxl.utils.datetime.from_excel(date)\n",
    "    list.append(newdate)\n",
    "pts['datetime'] = list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Drop unwanted columns from dataframe\n",
    "\n",
    "There are a lot of columns we do not need for our analysis, so we will drop these\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts.drop(columns = ['Date', 'Time', 'Timestamp','Reed 0',\n",
    "       'Reed 1', 'Reed 2', 'Reed 3', 'Battery Voltage', \n",
    "       'PRT Ref Voltage','SGS Voltage', 'Internal Temp 1', \n",
    "       'Internal Temp 2', 'Internal Temp 3','Cal Temp', \n",
    "       'Error Code 1', 'Error Code 2', 'Error Code 3',\n",
    "       'Records Saved', 'Bad Pages',], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 Correct the data types\n",
    "\n",
    "Run the cell below and note that the data types are 'object'. This is because we had a mix of text and numeric when importing these data (remember the units row). We convert the dtype to numeric using the pd.to_numeric method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts[\n",
    "    ['depth_m', 'speed_mps','cweight_kg','whp_barg','temp_degC','pressure_bara','frequency_hz']\n",
    "] = pts[\n",
    "    ['depth_m','speed_mps','cweight_kg','whp_barg','temp_degC','pressure_bara','frequency_hz']\n",
    "    ].apply(pd.to_numeric)\n",
    "\n",
    "pts.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## We are finished importing and munging the data!\n",
    "\n",
    "We now have flowrate and pts dataframes that we can work with for the rest of our analysis. \n",
    "\n",
    "The steps taken above to import and munge the data are wrapped into custom functions in uitlities.py. \n",
    "\n",
    "In the rest of this tutorial, we will call these functions rather than stepping through the method again. \n",
    "\n",
    "> from uitlities import*\n",
    "\n",
    "> flowrate = read_flowrate(r'PTS-2-injection-rate.xlsx')\n",
    "\n",
    "> pts = read_pts(r'PTS-2.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# 3. Overview of the completion test data\n",
    "\n",
    "Katie is making a richer plot for here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4, ax5, ax6) = plt.subplots(6, 1,figsize=(10,15),sharex=True)\n",
    "ax1.set_title('Entire completion test dataset',y=1.1,fontsize=15)\n",
    "\n",
    "ax1.plot(flowrate.datetime, flowrate.flow_tph, label='Surface pump flowrate', \n",
    "    c='k', linewidth=0.8, marker='.')\n",
    "ax1.set_ylabel('Surface flowrate [t/hr]')\n",
    "ax1.set_ylim(0,150)\n",
    " \n",
    "ax2.plot(pts.datetime, pts.depth_m, label='PTS tool depth', \n",
    "    c='k', linewidth=0.8)\n",
    "ax2.set_ylabel('PTS tool depth [m]')\n",
    "ax2.set_ylim(1000,0)\n",
    " \n",
    "ax3.plot(pts.datetime, pts.pressure_bara, label='PTS pressure', \n",
    "    c='tab:blue', linewidth=0.8)\n",
    "ax3.set_ylabel('PTS pressure [bara]')\n",
    " \n",
    "ax4.plot(pts.datetime, pts.temp_degC, label='PTS temperature', \n",
    "    c='tab:red', linewidth=0.8)\n",
    "ax4.set_ylabel('PTS temperature')\n",
    " \n",
    "ax5.plot(pts.datetime, pts.frequency_hz, label='PTS impeller frequency', \n",
    "    c='tab:green', linewidth=0.8)\n",
    "ax5.set_ylim(-30,30)\n",
    "ax5.set_ylabel('PTS impeller frequency [hz]')\n",
    "# 1 hz = 60 rpm\n",
    "\n",
    "ax6.plot(pts.datetime, pts.speed_mps, label='PTS tool speed', \n",
    "    c='tab:orange', linewidth=0.8)\n",
    "ax6.set_ylim(-2,2)\n",
    "ax6.set_ylabel('PTS tool speed [mps]')\n",
    " \n",
    "ax6.set_xlabel('Time [hh:mm]')\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    " \n",
    "for ax in [ax1,ax2,ax3,ax4,ax5,ax6]:\n",
    "    ax.grid()\n",
    " \n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "The next notebook in this tutoral is 2-temperature.ipynb\n",
    "\n",
    "***\n",
    "\n",
    "<p><center>Â© 2020 <a href=\"https://www.cubicearth.nz/\">Irene Wallis</a> and <a href=\"https://www.linkedin.com/in/katie-mclean-25994315/\">Katie McLean</a> <a href=\"https://creativecommons.org/licenses/by/4.0/\">CC-BY</a></center></p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}